# DEND Project 1: Data Modeling with Postgres

## Project Summary

The objective of this project is to create a SQL analytics database for a fictional music streaming service called Sparkify. Sparkify's analytics team seeks to understand what, when and how users are playing songs on the company's music app. The analysts need an easy way to query and analyze the songplay data, which is currently stored in raw JSON logs and metadata files on a local directory.

As the data engineer assigned to the project, I have implemented an ETL pipeline in python to process and upload the data into a PostgreSQL database. The ETL process extracts each songplay from the list of page actions recorded by the app. Data for analysis, such as song name, user information, subscription tier, and location of user, is structured into the main songplay table and related dimensional tables. 

Data Modeling with Postgres was submitted for Udacity's Data Engineering Nanodegree (DEND) in Spring 2019.

## Database Schema

The sparkify database design uses the simple star schema shown below. The schema contains one fact table, *songplays*, and four dimension tables: *songs*, *artists*, *users* and *time*. The fact table references the primary keys of each dimention table, enabling joins to songplays on song_id, artist_id, user_id and start_time, respectively. This structure will enable the analysts to aggregate the data efficiently and explore it using standard SQL queries. 

![Database schema diagram](database_schema_diagram.png)

###### Instructions for generating the schema diagram using [sqlalchemy_schemadisplay](https://github.com/fschulze/sqlalchemy_schemadisplay) were provided by Syed Mateen in the project-1-dend-v1 slack channel. Thanks Syed!

Each songplay in the fact table is identified by a unique uuid generated from the song, user id and timestamp of the log entry. This field is set as a primary key, so that it is unique and non-null. A constraint on the UPSERT operation ensures that there are no duplicate songplays in the database. If the log contains multiple entries with the same song, user id and timestamp, only the first entry is imported. The process of generating unique uuid's could be applied to all of the primary identifiers of the dimension tables. This would improve join efficiency if the database were very large. 

## Data Processing and Quality Checks

Data is extracted from two types of JSON source files: song data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) and songplay data from user logs. The JSON files are read into pandas dataframes, processed and uploaded into the database using psycopg2. 

A number of steps clean the data and reduce the size of the database by removing data not needed for the analysis: 
* Songplays are identified by filtering for actions initiated from the 'NextSong' page. 
* Timestamps are converted from UNIX time to datetime format without time zone prior to upload.
* Rows from the users table are excluded where user_id is missing.
* Rows from the artists table are excluded where artist_id is missing. 

## Example Queries and Results

The dataset contains 6,820 songplays from November 2018.

> SELECT tm.month, tm.year, COUNT(sp.songplay_id) as songplay_count 
  FROM songplays sp 
  LEFT JOIN time tm 
   ON sp.start_time = tm.start_time 
  GROUP BY tm.month, tm.year;

81% of songplays -- 5591 streams -- are generated by paid members.

> SELECT 
    sp.level, 
    COUNT(sp.songplay_id) as songplay_count, 
    100*COUNT(sp.songplay_id)/(select count(s.songplay_id) from songplays s) as percent 
  FROM songplays sp GROUP BY sp.level;